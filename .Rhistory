de_tha_d <- read.csv('de_tha_d.csv')
usethis::use_data(de_tha_d)
usethis::use_data(de_tha_f)
usethis::use_data(de_tha_h)
load("C:/Users/pedro/OneDrive/@DOUTORADO/@@TU-Berlin/@Artigos/CAP 4/package/fdClassify/data/de_tha_d.rda")
de_tha_h
colmanes(de_tha_h)
colnames(de_tha_h)
colnames(de_tha_d)
library(fdClassify)
fd_FordLabosier <- FordLabosier2017(vtime = de_tha_d$time,
vswc = de_tha_d$soil_water,
crit = c(40,20,30))
Sys.setenv(lang = 'en')
fd_FordLabosier <- FordLabosier2017(vtime = de_tha_d$time,
vswc = de_tha_d$soil_water,
crit = c(40,20,30))
detach("package:fdClassify", unload = TRUE)
library(fdClassify)
fd_FordLabosier <- FordLabosier2017(vtime = de_tha_d$time,
vswc = de_tha_d$soil_water,
crit = c(40,20,30))
a <- date
a
a <- "2020-01-01"
a <- as.Date(a)
a
year(a)
library(lubridate)
year(a)
fd_FordLabosier <- FordLabosier2017(vtime = de_tha_d$time,
vswc = de_tha_d$soil_water,
crit = c(40,20,30))
detach("package:lubridate", unload = TRUE)
detach("package:fdClassify", unload = TRUE)
library(fdClassify)
fd_FordLabosier <- FordLabosier2017(vtime = de_tha_d$time,
vswc = de_tha_d$soil_water,
crit = c(40,20,30))
a <- "2020-01-01"
fd_FordLabosier <- FordLabosier2017(vtime = de_tha_d$time,
vswc = de_tha_d$soil_water,
crit = c(40,20,30))
library(lubridate)
fd_FordLabosier <- FordLabosier2017(vtime = de_tha_d$time,
vswc = de_tha_d$soil_water,
crit = c(40,20,30))
vtime = de_tha_d$time
vswc = de_tha_d$soil_water
crit = c(40,20,30))
crit = c(40,20,30)
crit1 = crit[1] #upper limit
crit2 = crit[2] #lower limit
crit3 = crit[3] # recuperation limit
swc <- data.frame(time = vtime, swc = vswc)
#get pentads
pentad.swc.list <- f.pentad(vtime = swc$time, vvalue = swc$swc,
na_rm = F, f = mean)
series.swc <- pentad.swc.list$pentad_timestamp
pentad.swc <- pentad.swc.list$pentad_matrix
# get percentiles
percentile.swc <- t(apply(pentad.swc,1, f.percentile))
ts.percentile.swc <- ts(c(percentile.swc), frequency = 73, start =  min(year(series.swc$time)))
#get column of percentiles
percentile.series <- c(percentile.swc)
#remove NA from the beggining of the series. Necessary for p.min calculation.
firstNonNA <- min(which(!is.na(percentile.series)))
percentile.series <- percentile.series[firstNonNA:length(percentile.series)]
#get accumulated difference from 1 to 4 pentads.
a1 <- unlist(lapply(1:length(percentile.series),
function(i) percentile.series[i] - percentile.series[i-1])) %>%
c(rep(NA,1),.)
a2 <- unlist(lapply(2:length(percentile.series),
function(i) percentile.series[i] - percentile.series[i-2])) %>%
c(rep(NA,2),.)
library(dplyr)
vtime = de_tha_d$time
vswc = de_tha_d$soil_water
crit = c(40,20,30)
crit1 = crit[1] #upper limit
crit2 = crit[2] #lower limit
crit3 = crit[3] # recuperation limit
swc <- data.frame(time = vtime, swc = vswc)
#get pentads
pentad.swc.list <- f.pentad(vtime = swc$time, vvalue = swc$swc,
na_rm = F, f = mean)
series.swc <- pentad.swc.list$pentad_timestamp
pentad.swc <- pentad.swc.list$pentad_matrix
# get percentiles
percentile.swc <- t(apply(pentad.swc,1, f.percentile))
ts.percentile.swc <- ts(c(percentile.swc), frequency = 73, start =  min(year(series.swc$time)))
# get percentiles
percentile.swc <- t(apply(pentad.swc,1, f.percentile))
swc <- data.frame(time = vtime, swc = vswc)
#get pentads
pentad.swc.list <- f.pentad(vtime = swc$time, vvalue = swc$swc,
na_rm = F, f = mean)
vtime
de_tha_d.csv
de_tha_d <- read.csv('de_tha_d.csv')
de_tha_d
de_tha_d <- de_tha_d[,-c(1)]
de_tha_d
de_tha_d$time <- ymd(de_tha_d$time)
de_tha_d
de_tha_h <- read.csv('de_tha_h.csv')
de_tha_h <- de_tha_h[,-c(1)]
de_tha_h$time <- ymd_hm(de_tha_h$time)
de_tha_h
usethis::use_data(de_tha_d)
usethis::use_data(de_tha_d,overwrite = TRUE)
usethis::use_data(de_tha_h,overwrite = TRUE)
pacman::p_load('ncdf4','ncdf4.helpers','PCICt','ggplot2','tidyr','dplyr','readr'
,'raster','tibbletime','lubridate','RColorBrewer','stringr','knitr'
,'tinytex','data.table','runner','reshape2', 'tidyquant','SPEI'
, 'ggforce','tidyverse','tictoc')
fd_FordLabosier <- FordLabosier2017(vtime = de_tha_d$time,
vswc = de_tha_d$soil_water,
crit = c(40,20,30))
fd_FordLabosier
fd_Mo <- Mo2016(vtime = de_tha_d$time, vprecipitation = de_tha_d$precipitation,
vtemperature = de_tha_d$temperature, vsoil_water = de_tha_d$soil_water,
vlatent_heat = de_tha_d$latent_heat)
detach("package:fdClassify", unload = TRUE)
library(fdClassify)
fd_Mo <- Mo2016(vtime = de_tha_d$time, vprecipitation = de_tha_d$precipitation,
vtemperature = de_tha_d$temperature, vsoil_water = de_tha_d$soil_water,
vlatent_heat = de_tha_d$latent_heat)
library(fdClassify)
fd_Mo <- Mo2016(vtime = de_tha_d$time, vprecipitation = de_tha_d$precipitation,
vtemperature = de_tha_d$temperature, vsoil_water = de_tha_d$soil_water,
vlatent_heat = de_tha_d$latent_heat)
library(fdClassify)
de_tha_d
library(fdClassify)
library(fdClassify)
library(fdClassify)
remove.packages("fdClassify", lib="~/R/win-library/4.0")
install.packages('fdClassify')
install.packages("fdClassify")
library(fdClassify)
if(!require('pacman'))install.packages('pacman')
pacman::p_load('ncdf4','ncdf4.helpers','PCICt','ggplot2','tidyr','dplyr','readr'
,'raster','tibbletime','lubridate','RColorBrewer','stringr','knitr'
,'tinytex','data.table','runner','reshape2', 'tidyquant','SPEI'
, 'ggforce','tidyverse','tictoc')
Sys.setenv(lang = 'en')
fd_FordLabosier <- FordLabosier2017(vtime = de_tha_d$time,
vswc = de_tha_d$soil_water,
crit = c(40,20,30))
fd_Mo <- Mo2016(vtime = de_tha_d$time, vprecipitation = de_tha_d$precipitation,
vtemperature = de_tha_d$temperature, vsoil_water = de_tha_d$soil_water,
vlatent_heat = de_tha_d$latent_heat)
Mo2016
de_tha_d
de_tha_d$time
vtime = de_tha_d$time
vprecipitation = de_tha_d$precipitation
vtemperature = de_tha_d$temperature
vsoil_water = de_tha_d$soil_water
vlatent_heat = de_tha_d$latent_heat
flux_data = T
if (flux_data){
vevap <- actual_evap_day(vtime = vtime, vlatent_heat = vlatent_heat,
vtemperature = vtemperature)
#remove negative values (condensation)
vevap[vevap < 0] <- 0
} else {vevap[vevap < 0] <- 0}
#get dataa into list
list_classification <- list(precipitation = data.frame(time = vtime,
value = vprecipitation),
temperature = data.frame(time = vtime,
value = vtemperature),
soil_water = data.frame(time = vtime,
value = vsoil_water),
actual_evap = data.frame(time = vtime,
value = vevap))
list_classification
vevap
#get dataa into list
list_classification <- list(precipitation = data.frame(time = vtime,
value = vprecipitation),
temperature = data.frame(time = vtime,
value = vtemperature),
soil_water = data.frame(time = vtime,
value = vsoil_water),
actual_evap = data.frame(time = vtime,
value = vevap$eta))
list_classification
list_pentad <- NULL
var_names <- c('precipitation', 'temperature','soil_water', 'actual_evap')
for (i in var_names){
list_pentad[[i]] <- f.pentad(vtime = list_classification[[i]]$time,
vvalue = list_classification[[i]]$value)
}
list_pentad
detach("package:fdClassify", unload = TRUE)
library(fdClassify)
fd_Mo <- Mo2016
Mo2016
library(fdClassify)
library(fdClassify)
library(fdClassify)
library(fdClassify)
aux_year <- lubridate::year
library(fdClassify)
esr_pentad <- f.pentad(vtime = data_et$time, vvalue = data_et$esr, na_rm = T)
library(fdClassify)
library(fdClassify)
#load required packages
library('tidyr')
library('tidyr')
library('dplyr')
library('readr')
library('tibbletime')
library('lubridate')
library('stringr')
library('runner')
library(fdClassify)
library(fdClassify)
library(fdClassify)
library(fdClassify)
Sys.setenv(lang - 'en')
Sys.setenv(lang = 'en')
library(fdClassify)
library(fdClassify)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
if(!require('pacman'))install.packages('pacman')
pacman::p_load('ncdf4','ncdf4.helpers','PCICt','ggplot2','tidyr','dplyr','readr'
,'raster','tibbletime','lubridate','RColorBrewer','stringr','knitr'
,'tinytex','data.table','runner', 'reshape2', 'tidyquant','SPEI'
, 'ggforce')
pacman::p_load('fdClassify')
WD <- "C:/Users/pedro/OneDrive/@DOUTORADO/@@TU-Berlin/@Artigos/CAP 4/R"
setwd(WD)
de.tha <- read.csv('FLX_DE-Tha_FLUXNET2015_SUBSET_DD_1996-2014_1-4.csv')
de.tha <- read.csv('FLX_DE-Tha_FLUXNET2015_SUBSET_DD_1996-2014_1-4.csv')
df <- de.tha
data_et <- df[,c('TIMESTAMP','WS_F','TA_F', 'H_F_MDS', 'LE_F_MDS', 'VPD_F')]
write.csv(data_et,'de_tha_et.csv')
data_et$TIMESTAMP <- ymd(data_et$TIMESTAMP)
data_et <- na_if(data_et,-9999)
colnames(data_et) <- c('time', 'wind_speed', 'temperature', 'sensible_heat', 'latent_heat', 'vapor_p_def')
## N3cessary conversions
data_et$vapor_p_def <- 0.1*data_et$vapor_p_def
data_et$heat_flux <- (data_et$sensible_heat +
data_et$latent_heat)/86.4
et0 <- penman_day(v.time = data_et$time, v.wind = data_et$wind_speed,
v.temp = data_et$temperature, v.vpd =data_et$vapor_p_def,
v.heatflux = data_et$heat_flux)
et0 <- penman_day(vtime = data_et$time, vwind = data_et$wind_speed,
vtemp = data_et$temperature, vvpd =data_et$vapor_p_def,
vheatflux = data_et$heat_flux)
et0$time <- as.Date(et0$time)
week.et0.list <- f.week(et0)
series.et0 <- week.et0.list$week_timestamp
week.et0 <- week.et0.list$week_matrix
# get percentiles
# we used the here the percentiles of EDDI as described in Hoggins 2016.
# Pendergrass is not quite clear on how to calculate the EDDI. What does a
# 50% drop means? a drop in percentile or in absolute eddi?
percentile.et0 <- t(apply(week.et0,1, eddi))
percentile.et0
data.table <- data.frame(time = as.Date(series.et0$time),
percentile = c(percentile.et0),
et0 = c(week.et0))
data.table$dif_perc <- append(NA,as.vector(diff(data.table$percentile,
lag = 1)))
limit.down = 5
et0$time <- as.Date(et0$time)
week.et0.list <- f.week(et0)
series.et0 <- week.et0.list$week_timestamp
week.et0 <- week.et0.list$week_matrix
# get percentiles
# we used the here the percentiles of EDDI as described in Hoggins 2016.
# Pendergrass is not quite clear on how to calculate the EDDI. What does a
# 50% drop means? a drop in percentile or in absolute eddi?
percentile.et0 <- t(apply(week.et0,1, eddi))
# get percentiles
# we used the here the percentiles of EDDI as described in Hoggins 2016.
# Pendergrass is not quite clear on how to calculate the EDDI. What does a
# 50% drop means? a drop in percentile or in absolute eddi?
percentile.et0 <- t(apply(week.et0,1, eddi_percentile))
percentile.et0
data.table <- data.frame(time = as.Date(series.et0$time),
percentile = c(percentile.et0),
et0 = c(week.et0))
data.table$dif_perc <- append(NA,as.vector(diff(data.table$percentile,
lag = 1)))
#Remove eventual NA in the beginning of the series
firstNonNA <- min(which(!is.na(data.table$percentile)))
data.table <- data.table[firstNonNA:nrow(data.table),]
data.table$delta <- unlist(lapply(4:nrow(data.table),
function(i) data.table$percentile[i] -
data.table$percentile[i-3])) %>%   c(rep(NA,3),.)
data.table
data.table$delta <- unlist(lapply(4:nrow(data.table),
function(i) data.table$percentile[i] -
data.table$percentile[i-2])) %>%   c(rep(NA,2),.)
data.table$delta <- unlist(lapply(4:nrow(data.table),
function(i) data.table$percentile[i] -
data.table$percentile[i-2])) %>%   c(rep(NA,2),.)
data.table$delta <- unlist(lapply(3:nrow(data.table),
function(i) data.table$percentile[i] -
data.table$percentile[i-2])) %>%   c(rep(NA,2),.)
data.table
#Classification
data.table$is.fd <- 0
limit.downwards <- limit.down#max recuperation over sustain period
limit.downwards
for (i in 3:(nrow(data.table)-2)){
data.table$is.fd[i] <- (data.table$delta[i] >= 50) *
(data.table$percentile[i+1] - data.table$percentile[i] <= limit.downwards) *
(data.table$percentile[i+2] - data.table$percentile[i] <= limit.downwards)
}
data.table
for (i in 3:(nrow(data.table)-2)){
data.table$is.fd[i] <- (data.table$delta[i] >= 50) *
(data.table$percentile[i+1] - data.table$percentile[i] > -limit.downwards) *
(data.table$percentile[i+2] - data.table$percentile[i] > -limit.downwards)
}
data.table
View(data.table)
data.table$dif_perc <- append(c(NA,NA),as.vector(diff(data.table$percentile,
lag = 2)))
limit.down = 5
et0$time <- as.Date(et0$time)
week.et0.list <- f.week(et0)
series.et0 <- week.et0.list$week_timestamp
week.et0 <- week.et0.list$week_matrix
# get percentiles
# we used the here the percentiles of EDDI as described in Hoggins 2016.
# Pendergrass is not quite clear on how to calculate the EDDI. What does a
# 50% drop means? a drop in percentile or in absolute eddi?
percentile.et0 <- t(apply(week.et0,1, eddi_percentile))
data.table <- data.frame(time = as.Date(series.et0$time),
percentile = c(percentile.et0),
et0 = c(week.et0))
data.table$dif_perc <- append(c(NA,NA),as.vector(diff(data.table$percentile,
lag = 2)))
#Remove eventual NA in the beginning of the series
firstNonNA <- min(which(!is.na(data.table$percentile)))
data.table <- data.table[firstNonNA:nrow(data.table),]
#Classification
data.table$is.fd <- 0
limit.downwards <- limit.down#max recuperation over sustain period
for (i in 3:(nrow(data.table)-2)){
data.table$is.fd[i] <- (data.table$delta[i] >= 50) *
(data.table$percentile[i+1] - data.table$percentile[i] > -limit.downwards) *
(data.table$percentile[i+2] - data.table$percentile[i] > -limit.downwards)
}
for (i in 3:(nrow(data.table)-2)){
data.table$is.fd[i] <- (data.table$dif_perc[i] >= 50) *
(data.table$percentile[i+1] - data.table$percentile[i] > -limit.downwards) *
(data.table$percentile[i+2] - data.table$percentile[i] > -limit.downwards)
}
#get correct durations
for (i in 3:(nrow(data.table)-1)){
if (data.table$is.fd[i] ==1){
# limit <- data.table$percentile[i] - limit.downwards
data.table$is.fd[i-1] = data.table$is.fd[i-2] <- 1
if (data.table$dif_perc[i+1] < limit.upwards){
data.table$is.fd[i+1] <- 1
}
}
}
#get correct durations
for (i in 3:(nrow(data.table)-1)){
if (data.table$is.fd[i] ==1){
# limit <- data.table$percentile[i] - limit.downwards
data.table$is.fd[i-1] = data.table$is.fd[i-2] <- 1
if (data.table$dif_perc[i+1] < limit.downwards){
data.table$is.fd[i+1] <- 1
}
}
}
limit.down = 5
et0$time <- as.Date(et0$time)
week.et0.list <- f.week(et0)
series.et0 <- week.et0.list$week_timestamp
week.et0 <- week.et0.list$week_matrix
# get percentiles
# we used the here the percentiles of EDDI as described in Hoggins 2016.
# Pendergrass is not quite clear on how to calculate the EDDI. What does a
# 50% drop means? a drop in percentile or in absolute eddi?
percentile.et0 <- t(apply(week.et0,1, eddi_percentile))
data.table <- data.frame(time = as.Date(series.et0$time),
percentile = c(percentile.et0),
et0 = c(week.et0))
data.table$dif_perc <- append(c(NA,NA),as.vector(diff(data.table$percentile,
lag = 2)))
#Remove eventual NA in the beginning of the series
firstNonNA <- min(which(!is.na(data.table$percentile)))
data.table <- data.table[firstNonNA:nrow(data.table),]
# data.table$delta <- NA
# data.table$delta <- unlist(lapply(3:nrow(data.table),
#                                   function(i) data.table$percentile[i] -
#                                     data.table$percentile[i-2])) %>%   c(rep(NA,2),.)
#Classification
data.table$is.fd <- 0
limit.downwards <- limit.down#max recuperation over sustain period
for (i in 3:(nrow(data.table)-2)){
data.table$is.fd[i] <- (data.table$dif_perc[i] >= 50) *
(data.table$percentile[i+1] - data.table$percentile[i] > -limit.downwards) *
(data.table$percentile[i+2] - data.table$percentile[i] > -limit.downwards)
}
limit.down = 5
et0$time <- as.Date(et0$time)
week.et0.list <- f.week(et0)
series.et0 <- week.et0.list$week_timestamp
week.et0 <- week.et0.list$week_matrix
# get percentiles
# we used the here the percentiles of EDDI as described in Hoggins 2016.
# Pendergrass is not quite clear on how to calculate the EDDI. What does a
# 50% drop means? a drop in percentile or in absolute eddi?
percentile.et0 <- t(apply(week.et0,1, eddi_percentile))
data.table <- data.frame(time = as.Date(series.et0$time),
percentile = c(percentile.et0),
et0 = c(week.et0))
data.table$dif_perc <- append(c(NA,NA),as.vector(diff(data.table$percentile,
lag = 2)))
#Remove eventual NA in the beginning of the series
firstNonNA <- min(which(!is.na(data.table$percentile)))
data.table <- data.table[firstNonNA:nrow(data.table),]
# data.table$delta <- NA
# data.table$delta <- unlist(lapply(3:nrow(data.table),
#                                   function(i) data.table$percentile[i] -
#                                     data.table$percentile[i-2])) %>%   c(rep(NA,2),.)
#Classification
data.table$is.fd <- 0
limit.downwards <- limit.down#max recuperation over sustain period
for (i in 3:(nrow(data.table)-2)){
data.table$is.fd[i] <- (data.table$dif_perc[i] >= 50) *
(data.table$percentile[i+1] - data.table$percentile[i] > -limit.downwards) *
(data.table$percentile[i+2] - data.table$percentile[i] > -limit.downwards)
}
View(data.table)
#get correct durations
for (i in 3:(nrow(data.table)-1)){
if (data.table$is.fd[i] ==1 & data.table$is.fd[i-1] ==0){
limit <- data.table$percentile[i] - limit.downwards
while (data.table$percentile[i+1] >= limit){
data.table$is.fd[i+1] <- 1
i = i+1
}
}
}
limit.down = 5
et0$time <- as.Date(et0$time)
week.et0.list <- f.week(et0)
series.et0 <- week.et0.list$week_timestamp
week.et0 <- week.et0.list$week_matrix
# get percentiles
# we used the here the percentiles of EDDI as described in Hoggins 2016.
# Pendergrass is not quite clear on how to calculate the EDDI. What does a
# 50% drop means? a drop in percentile or in absolute eddi?
percentile.et0 <- t(apply(week.et0,1, eddi_percentile))
data.table <- data.frame(time = as.Date(series.et0$time),
percentile = c(percentile.et0),
et0 = c(week.et0))
data.table$dif_perc <- append(c(NA,NA),as.vector(diff(data.table$percentile,
lag = 2)))
#Remove eventual NA in the beginning of the series
firstNonNA <- min(which(!is.na(data.table$percentile)))
data.table <- data.table[firstNonNA:nrow(data.table),]
# data.table$delta <- NA
# data.table$delta <- unlist(lapply(3:nrow(data.table),
#                                   function(i) data.table$percentile[i] -
#                                     data.table$percentile[i-2])) %>%   c(rep(NA,2),.)
#Classification
data.table$is.fd <- 0
limit.downwards <- limit.down#max recuperation over sustain period
for (i in 3:(nrow(data.table)-2)){
data.table$is.fd[i] <- (data.table$dif_perc[i] >= 50) *
(data.table$percentile[i+1] - data.table$percentile[i] > -limit.downwards) *
(data.table$percentile[i+2] - data.table$percentile[i] > -limit.downwards)
}
View(data.table)
#get correct durations
for (i in 3:(nrow(data.table)-1)){
if (data.table$is.fd[i] ==1 & data.table$is.fd[i-1] ==0){
data.table$is.fd[i-1] = 1
data.table$is.fd[i-2] = 1
limit <- data.table$percentile[i] - limit.downwards
while (data.table$percentile[i+1] >= limit){
data.table$is.fd[i+1] <- 1
i = i+1
}
}
}
limit.down = 10
#Classification
data.table$is.fd <- 0
limit.downwards <- limit.down#max recuperation over sustain period
for (i in 3:(nrow(data.table)-2)){
data.table$is.fd[i] <- (data.table$dif_perc[i] >= 50) *
(data.table$percentile[i+1] - data.table$percentile[i] > -limit.downwards) *
(data.table$percentile[i+2] - data.table$percentile[i] > -limit.downwards)
}
View(data.table)
#get correct durations
for (i in 3:(nrow(data.table)-1)){
if (data.table$is.fd[i] ==1 & data.table$is.fd[i-1] ==0){
data.table$is.fd[i-1] = 1
data.table$is.fd[i-2] = 1
limit <- data.table$percentile[i] - limit.downwards
while (data.table$percentile[i+1] >= limit){
data.table$is.fd[i+1] <- 1
i = i+1
}
}
}
