series_sesr_selec$av_change_perc <- NA
for (i in 1:n_events){
# i = 1
#identify potential FD location in the time series
year_col <- series_sesr_selec$year[i] - min(series_sesr$year) + 1 # set index from 1 to nyear
duration <- series_sesr_selec$dur[i]
p_final <- series_sesr_selec$pentad[i] + 73
p_inicial <- p_final - duration + 1
# Assess percentile
av_frame_delta_sesr <- colMeans(pentad.delta_sesr_aux[p_inicial:p_final,], na.rm = T)
series_sesr_selec$av_change_perc[i] <- f.percentile(av_frame_delta_sesr)[year_col]
# print(i)
}
series_sesr_selec <- series_sesr_selec[series_sesr_selec$av_change_perc <= 30,]
# Remove redundant events (if any)
for (i in 2:nrow(series_sesr_selec)){
if (series_sesr_selec$time[i-1] >= series_sesr_selec$time[i] - 86400*5*series_sesr_selec$dur[i]){
series_sesr_selec[i-1,] <- NA
}
}
series_sesr_selec <- series_sesr_selec[complete.cases(series_sesr_selec),]
series_sesr_selec_duration <- series_sesr_selec[,c(1,9)]
series_sesr_output <- series_sesr[,-c(9,10)]
series_sesr_output <- dplyr::left_join(series_sesr_output,series_sesr_selec_duration, by = 'time')
series_sesr_output$dur[is.nan(series_sesr_output$dur)] <- NA
#recompose the time series with NA
if(nrow(bkp > 0)){
bkp$dur <- NA
series_sesr_output <- rbind(bkp,series_sesr_output)
}
bkp
class(bkp)
as.matrix(bkp)
as.data.frame(as.matrix(bkp))
nrow(as.matrix(bkp))
R.version()
version
install.packages("installr")
install.packages('devtools') #assuming it is not already installed
library(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
1
updateR()
updateR(admin_password = '112358')
updateR(admin_password = '1123')
version
version
ET0 <- penman_day(vtime = de_tha_d$time, vwind = de_tha_d$wind_speed,
vvpd = de_tha_d$vapor_p_def*10, vtemp = de_tha_d$temperature,
vheatflux = (de_tha_d$sensible_heat+de_tha_d$latent_heat))
install.packages("devtools")
install_github("pedroalencar1/fdClassify")
library(devtools)
install_github("pedroalencar1/fdClassify")
library(fdClassify)
install.packages("ncdf4")
install_github("cran/ncdf4")
ncdf4
library(ncdf4)
install.packages(ncdf4)
install.packages("ncdf4")
library(fdClassify)
install_github("pedroalencar1/fdClassify")
library(fdClassify)
ET0 <- penman_day(vtime = de_tha_d$time, vwind = de_tha_d$wind_speed,
vvpd = de_tha_d$vapor_p_def*10, vtemp = de_tha_d$temperature,
vheatflux = (de_tha_d$sensible_heat+de_tha_d$latent_heat))
ETa <- actual_evap_day(vtime = de_tha_d$time, vlatent_heat = de_tha_d$latent_heat,
vtemperature = de_tha_d$temperature)
input_Christian <- Christian2020_clean_data(vtime = de_tha_d$time,
vET0 = ET0$et0, vETa = ETa$eta,
threshold = 1)
esr_list <- input_Christian
series.esr <- esr_list[[1]]
pentad.esr <- esr_list[[2]]
# 1) get SESR and variations into pentads
# ger SESR values and percentiles
pentad.sesr_value <- t(apply(pentad.esr,1, f.anomaly))
pentad.sesr_percentile <- t(apply(pentad.sesr_value,1, f.percentile))
#get de Delta.SESR values and percentiles
#  - first we get the differences using the diff function.
pentad.delta_sesr_series <- c(pentad.sesr_value) %>% diff(lag = 1) %>% append(NA,.)
# - Now we can get the Delta.SESR values and percentiles.
### - get the data in matrix format (cols with years)
pentad.delta_sesr <- matrix(pentad.delta_sesr_series, nrow = nrow(pentad.sesr_value),
ncol =  ncol(pentad.sesr_value), byrow = F)
library(dplyr)
# 1) get SESR and variations into pentads
# ger SESR values and percentiles
pentad.sesr_value <- t(apply(pentad.esr,1, f.anomaly))
pentad.sesr_percentile <- t(apply(pentad.sesr_value,1, f.percentile))
#get de Delta.SESR values and percentiles
#  - first we get the differences using the diff function.
pentad.delta_sesr_series <- c(pentad.sesr_value) %>% diff(lag = 1) %>% append(NA,.)
# - Now we can get the Delta.SESR values and percentiles.
### - get the data in matrix format (cols with years)
pentad.delta_sesr <- matrix(pentad.delta_sesr_series, nrow = nrow(pentad.sesr_value),
ncol =  ncol(pentad.sesr_value), byrow = F)
max(pentad.delta_sesr, na.rm = T)
pentad.delta_sesr_value <- t(apply(pentad.delta_sesr,1, f.anomaly))
pentad.delta_sesr_percentile <- t(apply(pentad.delta_sesr_value,1, f.percentile))
########## set unique dataframe
series_sesr <- data.frame(time = series.esr$time, esr_value = series.esr$var,
sesr_value = c(pentad.sesr_value),
sesr_perc = c(pentad.sesr_percentile),
d.sesr_value = c(pentad.delta_sesr_value),
d.sesr_perc = c(pentad.delta_sesr_percentile))
# 2) apply rules 1 to 3
#get years and pentads
series_sesr$year <- lubridate::year(series_sesr$time)
series_sesr$pentad <- rep(1:73,(max(series_sesr$year)-
min(series_sesr$year) + 1))
#remove NA values from the begining
firstNonNA <- min(which(!is.na(series_sesr$sesr_perc)))
bkp <- series_sesr[1:firstNonNA-1,]
series_sesr <- series_sesr[firstNonNA:nrow(series_sesr),]
######
# criterion 3 -> delta.SESR below 40 allowing 1 pentad recuperation
aux_delta <- (series_sesr$d.sesr_perc <= 40)*1
aux_delta[is.na(aux_delta)] <- 0
#get positions where criterion 3 is true
df_id <- data.frame(id_abs = rle(aux_delta)[[1]],
class = rle(aux_delta)[[2]])
# remove contiguous events and update indexes
df_id$id_acc <- runner::runner(df_id$id_abs, sum)
df_id$id_new <- df_id$id_abs
for (i in seq(4,nrow(df_id),2)){
if(df_id$id_abs[i-1] == 1){
df_id$id_new[i] <- df_id$id_abs[i] + df_id$id_new[i-2] + 1
df_id$id_new[i-2] <- 0
df_id$id_new[i-1] <- 0
}
}
df_id$id_acc2 <- runner::runner(df_id$id_new, sum)
# criterion 1 -> at least six pentads duration
df_id_fd <- df_id[(df_id$id_new >= 6 & df_id$class == 1),]
fd_events <- series_sesr[df_id_fd$id_acc2,]
fd_events$dur <- df_id_fd$id_new
# criterion 2 -> final SESR percentile lower than 20th.
series_sesr_selec <- fd_events[fd_events$sesr_perc <= 20,]
#############################################################################
# 3) apply rule 4
n_events <- nrow(series_sesr_selec)
# set an auxiliar pentad matrix - simple way to avoid errors in FD occurring
#   in the beginning of years
pentad.delta_sesr_aux <- rbind(pentad.delta_sesr, pentad.delta_sesr)
pentad.sesr_percentile <- t(apply(pentad.sesr_value,1, f.percentile))
series_sesr_selec$av_change_perc <- NA
for (i in 1:n_events){
# i = 1
#identify potential FD location in the time series
year_col <- series_sesr_selec$year[i] - min(series_sesr$year) + 1 # set index from 1 to nyear
duration <- series_sesr_selec$dur[i]
p_final <- series_sesr_selec$pentad[i] + 73
p_inicial <- p_final - duration + 1
# Assess percentile
av_frame_delta_sesr <- colMeans(pentad.delta_sesr_aux[p_inicial:p_final,], na.rm = T)
series_sesr_selec$av_change_perc[i] <- f.percentile(av_frame_delta_sesr)[year_col]
# print(i)
}
series_sesr_selec <- series_sesr_selec[series_sesr_selec$av_change_perc <= 30,]
# Remove redundant events (if any)
for (i in 2:nrow(series_sesr_selec)){
if (series_sesr_selec$time[i-1] >= series_sesr_selec$time[i] - 86400*5*series_sesr_selec$dur[i]){
series_sesr_selec[i-1,] <- NA
}
}
series_sesr_selec <- series_sesr_selec[complete.cases(series_sesr_selec),]
series_sesr_selec_duration <- series_sesr_selec[,c(1,9)]
series_sesr_output <- series_sesr[,-c(9,10)]
series_sesr_output <- dplyr::left_join(series_sesr_output,series_sesr_selec_duration, by = 'time')
series_sesr_output$dur[is.nan(series_sesr_output$dur)] <- NA
#recompose the time series with NA
if(nrow(bkp > 0)){
bkp$dur <- NA
series_sesr_output <- rbind(bkp,series_sesr_output)
}
bkp
library(ncdf4)
ncdf4::nc_open("/Users/alencar/Downloads/adaptor.mars.internal-1673304466.2909036-16882-16-d44b84fa-6e29-4c99-9a1d-9ec18fbc3c8c.nc")
ncin <- ncdf4::nc_open("/Users/alencar/Downloads/adaptor.mars.internal-1673304466.2909036-16882-16-d44b84fa-6e29-4c99-9a1d-9ec18fbc3c8c.nc")
pixel_time <- nc.get.time.series(ncin, v = vname[1], time.dim.name = "time",correct.for.gregorian.julian = FALSE,
return.bounds = TRUE)
library(ncdf4.helpers)
pixel_time <- nc.get.time.series(ncin, v = vname[1], time.dim.name = "time",correct.for.gregorian.julian = FALSE,
return.bounds = TRUE)
pixel_time <- as.POSIXct(pixel_time, format="%Y-%m-%d %H:%M:S")
pixel_time
ncdf4::nc_open("/Users/alencar/Downloads/adaptor.mars.internal-1673304466.2909036-16882-16-d44b84fa-6e29-4c99-9a1d-9ec18fbc3c8c.nc") %>%
nc.get.time.series(v = vname[1], time.dim.name = "time",correct.for.gregorian.julian = FALSE,
return.bounds = TRUE) %>% as.POSIXct(format="%Y-%m-%d %H:%M:S") %>% head(1)
ncdf4::nc_open("/Users/alencar/Downloads/adaptor.mars.internal-1673304466.2909036-16882-16-d44b84fa-6e29-4c99-9a1d-9ec18fbc3c8c.nc") %>%
nc.get.time.series(v = vname[1], time.dim.name = "time",correct.for.gregorian.julian = FALSE,
return.bounds = TRUE) %>% as.POSIXct(format="%Y-%m-%d %H:%M:S") %>% head(1)
ncdf4::nc_open("/Users/alencar/Downloads/adaptor.mars.internal-1673303996.2968707-16124-17-3a424b4d-27d4-4be7-b434-fd71368c32c9.nc") %>%
nc.get.time.series(v = vname[1], time.dim.name = "time",correct.for.gregorian.julian = FALSE,
return.bounds = TRUE) %>% as.POSIXct(format="%Y-%m-%d %H:%M:S") %>% head(1)
ncdf4::nc_open("/Users/alencar/Downloads/adaptor.mars.internal-1673303704.17337-18966-15-6bda1533-5496-4977-b145-b0d47ed83e54.nc") %>%
nc.get.time.series(v = vname[1], time.dim.name = "time",correct.for.gregorian.julian = FALSE,
return.bounds = TRUE) %>% as.POSIXct(format="%Y-%m-%d %H:%M:S") %>% head(1)
ncdf4::nc_open("/Users/alencar/Downloads/adaptor.mars.internal-1673299636.8272185-16882-8-d5e2597d-2441-4258-81a2-83ef77a86ffe.nc") %>%
nc.get.time.series(v = vname[1], time.dim.name = "time",correct.for.gregorian.julian = FALSE,
return.bounds = TRUE) %>% as.POSIXct(format="%Y-%m-%d %H:%M:S") %>% head(1)
a <- 202112010010
a <- as.character(a)
a
year <- substr(a, 1,4)
year
year <- substr(a, 1,4)
month <- substr(a,5,6)
day <- substr(a,7,8)
hour <- substr(a,9,10)
minute <- substr(a,11,12)
date_time <- paste(year,"-", month, "-", day, " ", hour,":", minute)
date_time
date_time <- paste(year,"-", month, "-", day, " ", hour,":", minute, sep="")
date_time
date_time %>% as.Date()
library(dplyr)
date_time %>% as.Date()
date_time %>% lubridate::as_datetime()
date_time <- paste(year,"-", month, "-", day, sep="")
date_time %>% as.Date()
date_time + 0.1
library(fdClassify)
require(stats)
library(lubridate)
de_tha_d
df_d <- de_tha_d
head(df_d)
#
# as.Date(df_d, origin="1996-01-01") add date formats as
#
fd_Mo <- Mo2016(vtime = as.Date(df_d$time, origin="1996-01-01"), vprecipitation = df_d$precipitation,
vtemperature = df_d$temperature, vsoil_water = df_d$soil_water,
vlatent_heat = df_d$latent_heat, flux_data = T)
fd_Mo
flux_data
# remotes::install_github("pedroalencar1/fdClassify")
#
library(fdClassify)
require(stats)
library(lubridate)
#
setwd("/home/drought/R/fdClassify")
# remotes::install_github("pedroalencar1/fdClassify")
#
library(fdClassify)
require(stats)
library(lubridate)
#
# setwd("/home/drought/R/fdClassify")
df_d <- de_tha_d
head(df_d)
#
# as.Date(df_d, origin="1996-01-01") add date formats as
#
fd_Mo <- Mo2016(vtime = as.Date(df_d$time, origin="1996-01-01"), vprecipitation = df_d$precipitation,
vtemperature = df_d$temperature, vsoil_water = df_d$soil_water,
vlatent_heat = df_d$latent_heat, flux_data = T)
fd_Mo
install.packages(c("BH", "boot", "broom", "class", "codetools", "colorspace", "curl", "dbplyr", "dplyr", "e1071", "evaluate", "extrafont", "fansi", "fontawesome", "forcats", "foreign", "fs", "future", "gargle", "gdtools", "ggpp", "httpuv", "knitr", "lubridate", "markdown", "MASS", "Matrix", "naniar", "ncdf4", "nlme", "parallelly", "pbapply", "purrr", "ragg", "raster", "Rcpp", "RcppArmadillo", "RCurl", "recipes", "rmarkdown", "Rttf2pt1", "sass", "sourcetools", "sp", "spatial", "SPEI", "stringi", "survival", "terra", "tibbletime", "tidyr", "timechange", "tinytex", "tseries", "utf8", "vctrs", "visdat", "vroom", "xfun", "yaml"))
library(fdClassify)
require(stats)
library(lubridate)
#
# setwd("/home/drought/R/fdClassify")
df_d <- de_tha_d
head(df_d)
#
# as.Date(df_d, origin="1996-01-01") add date formats as
#
fd_Mo <- Mo2016(vtime = as.Date(df_d$time, origin="1996-01-01"), vprecipitation = df_d$precipitation,
vtemperature = df_d$temperature, vsoil_water = df_d$soil_water,
vlatent_heat = df_d$latent_heat, flux_data = T)
"
test
"
devtools::install_github("pedroalencar1/fdClassify")
devtools::install_github("pedroalencar1/fdClassify", force = TRUE)
library(fdClassify)
de_tha_d
df <- de_tha_d
df <- de_tha_d
df_d <- de_tha_d
Christian2020_clean_data(vtime = df_d$time,
vET0 = ET0$et0, vETa = ETa$eta,
threshold = 1)
ET0
devtools::install_github("pedroalencar1/fdClassify", force = TRUE)
library(fdClassify)
df_d <- de_tha_d
Christian2020_clean_data(vtime = df_d$time,
vET0 = ET0$et0, vETa = ETa$eta,
threshold = 1)
fdClassify::actual_evap_day(vtime =  df_d$time,
vlatent_heat = df_d$latent_heat,
vtemperature = df_d$temperature)
ET0 <- fdClassify::penman_day(vtime =  df_d$time,
vwind = df_d$wind_speed,
vvpd = df_d$vapor_p_def,
vheatflux = df_d$latent_heat + df_d$sensible_heat,
vtemp = df_d$temperature)
ET0
ETa <- fdClassify::actual_evap_day(vtime =  df_d$time,
vlatent_heat = df_d$latent_heat,
vtemperature = df_d$temperature)
ET0 <- fdClassify::penman_day(vtime =  df_d$time,
vwind = df_d$wind_speed,
vvpd = df_d$vapor_p_def,
vheatflux = df_d$latent_heat + df_d$sensible_heat,
vtemp = df_d$temperature)
fd_Christian <- Christian2020_clean_data(vtime = df_d$time,
vET0 = ET0$et0, vETa = ETa$eta,
threshold = 2) %>%
Christian2020()
library(magrittr)
ETa <- fdClassify::actual_evap_day(vtime =  df_d$time,
vlatent_heat = df_d$latent_heat,
vtemperature = df_d$temperature)
ET0 <- fdClassify::penman_day(vtime =  df_d$time,
vwind = df_d$wind_speed,
vvpd = df_d$vapor_p_def,
vheatflux = df_d$latent_heat + df_d$sensible_heat,
vtemp = df_d$temperature)
fd_Christian <- Christian2020_clean_data(vtime = df_d$time,
vET0 = ET0$et0, vETa = ETa$eta,
threshold = 2) %>%
Christian2020()
fd_Christian
veta = ETa$eta,
multicriteria_fd(vtime =  df_d$time,
vtemp = df_d$temperature,
vprec = df_d$precipitation,
vet0 = ET0$et0,
veta = ETa$eta,
score = 0.6, d_score = 0.2,thresholds = c(1, 0, 0, 0, -2, 50, 10, 30))
test_multicriteria_fd <- function(vtime, vtemp, vprec, vet0, veta, score = 0.6, d_score = 0.2,thresholds = c(1, 0, 0, 0, -2, 50, 10, 30)){
##### test data
# df_d <- de_tha_d
# score = 0.6
# d_score = 0.2
# thresholds = c(1, 0, 0, 0, -2, 50, 10, 30)
#
# ET0 <- penman_day(vtime = df_d$time, vwind = df_d$wind_speed, vtemp = df_d$temperature,
#                   vvpd = df_d$vapor_p_def, vheatflux = (df_d$sensible_heat + df_d$latent_heat))[,2]
# ETa <- actual_evap_day(vtime = df_d$time,vlatent_heat =df_d$latent_heat, vtemperature = df_d$temperature)[,2]
#
# vtime = df_d$time
# vtemp = df_d$temperature
# vprec = df_d$precipitation
# vet0 = ET0
# veta = ETa
#build basic data frame
df_day <- data.frame(time = vtime, temperature = vtemp, precipitation = vprec,
et0 = vet0, eta = veta)
#accumulate data into weeks
list_week_matrix <- list(NA) #list of matrixes for each variable
df_week <- f.week(df_day[,c(1,2)],na_rm = F, f = mean, kind = 'standard')[[1]][,1]
for (i in 2:ncol(df_day)){
list_week_matrix[[i-1]] <- f.week(df_day[,c(1,i)],na_rm = F, f = mean, kind = 'standard')[[2]]
df_week[,i] <- f.week(df_day[,c(1,i)],na_rm = F, f = mean, kind = 'standard')[[1]][,2]
}
df_week[,3:5] <- df_week[,3:5] * 7 #adjust to display sum instead of mean
colnames(df_week) <- colnames(df_day) # rename columns
names(list_week_matrix) <- colnames(df_day[,2:5])
#anomaly of weekly accumulation
df_week_anomalies <- df_week
list_week_anomalies <- list_week_matrix
for (i in 2:ncol(df_week)){
list_week_anomalies[[i-1]] <- t(apply(as.data.frame(list_week_matrix[[i-1]]),1, f.anomaly))
df_week_anomalies[,i] <- c(list_week_anomalies[[i-1]])
}
#join anomalies
df_week$temp_anomaly <- df_week_anomalies$temperature
df_week$prec_anomaly <- df_week_anomalies$precipitation
df_week$et0_anomaly <- df_week_anomalies$et0
df_week$eta_anomaly <- df_week_anomalies$eta
df_week$time <- as.Date(df_week$time) #cast times from dttm to Date
#anomaly of 4 week accumulation
# ??
# indexes
### SPEI
df_week$deficit <- df_week$precipitation - df_week$et0 #calculate hidrological deficit
spei_list <- f.spei(df_week$time, df_week$deficit, n = 4) #get accumulated deficit over four weeks
df_week$spei <- spei_list[[1]]$spei
df_week$deficit <- NULL #remove deficit column
#EDDI
df_week$eddi <- c(eddi_percentile(vtime = df_week$time,
vet0 = df_week$et0, dist = 'tukey')
)
#SESR
sesr_input_clean <- Christian_clean_data_week(vtime = df_day$time,
vET0 = df_day$et0,
vETa = df_day$eta,
threshold = thresholds[1])
df_week$sesr <- sesr_input_clean[[1]]$var
n_weeks <- 4
# accumulations and differences over intensification period
df_week$temp_anomaly_ac <- runner::runner(df_week$temp_anomaly, k = 4, f = sum)
df_week$prec_anomaly_ac <- runner::runner(df_week$prec_anomaly, k = 4, f = sum)
df_week$et0_anomaly_ac <- runner::runner(df_week$et0_anomaly, k = 4, f = sum)
df_week$eta_anomaly_ac <- runner::runner(df_week$eta_anomaly, k = 4, f = sum)
df_week$spei_dif <- c(rep(NA, n_weeks),diff(df_week$spei, lag = n_weeks))
df_week$eddi_dif1 <- c(rep(NA, n_weeks-2),diff(df_week$eddi, lag = n_weeks-2))
df_week$sesr_dif <- c(rep(NA, n_weeks),diff(df_week$sesr, lag = n_weeks))
#delta.SESR percentiles
df_week$sesr_dif_perc <- c(t(apply(matrix(df_week$sesr_dif, nrow = 52, byrow = F),1, f.percentile)))
#criteria
criteria_names <- c('time', 'temp_anomaly', 'prec_anomaly', 'et0_anomaly', 'eta_anomaly',
'spei', 'sesr', 'eddi',
'prec_anomaly_ac','et0_anomaly_ac','temp_anomaly_ac',
'spei_dif', 'eddi_dif1', 'sesr', 'sesr_dif_perc', 'sesr_dif')
df_criteria <- df_week[colnames(df_week) %in% criteria_names]
df_criteria$temp_anomaly <- (df_criteria$temp_anomaly > thresholds[2])*1
df_criteria$prec_anomaly <- (df_criteria$prec_anomaly < -thresholds[2])*1
df_criteria$et0_anomaly <- (df_criteria$et0_anomaly > thresholds[2])*1
df_criteria$eta_anomaly <- (df_criteria$eta_anomaly > thresholds[2])*1
df_criteria$spei <- (df_criteria$spei < thresholds[3])*1
df_criteria$eddi <- (df_criteria$eddi > 100*pnorm(thresholds[2],0,1))*1 #
df_criteria$sesr <- (df_criteria$sesr < thresholds[2])*1 #
df_criteria$temp_anomaly_ac <- (df_criteria$temp_anomaly_ac > thresholds[4])*1
df_criteria$prec_anomaly_ac <- (df_criteria$prec_anomaly_ac < thresholds[4])*1
df_criteria$et0_anomaly_ac <- (df_criteria$et0_anomaly_ac > thresholds[4])*1
# df_criteria$eta_anomaly_ac <- (df_criteria$eta_anomaly_ac > thresholds[4])*1
df_criteria$spei_dif <- (df_criteria$spei_dif < thresholds[5])*1
df_criteria$sesr_dif <- (df_criteria$sesr_dif < thresholds[5])*1 #
df_criteria$eddi_dif1 <- (df_criteria$eddi_dif1 < thresholds[6])*1
df_criteria$sesr_dif_perc <- (df_criteria$sesr_dif_perc < thresholds[8])*1 #
n_criteria <- ncol(df_criteria)-1
# hist(df_criteria$score)
#     #get scores and percentiles
df_criteria$score <- rowSums(df_criteria[,2:ncol(df_criteria)], na.rm = T)/n_criteria
df_criteria$score_percentile_global <- f.percentile(df_criteria$score)
df_criteria$score_percentile_period <- c(t(apply(matrix(df_criteria$score,
nrow = 52, byrow = F),1,
f.percentile)))
# ncol(df_criteria)
# View(df_criteria)
# classify as event
df_events <- df_criteria[,c(1,16,17,18)]
# df_events$is.fd <- (df_events$score_percentile_global > score)*1
diffs <- data.frame(time = df_events$time)
diffs$d1 <- c(0,diff(df_events$score, lag = 1))
diffs$d2 <- c(0,0,diff(df_events$score, lag = 2))
diffs$d3 <- c(0,0,0,diff(df_events$score, lag = 3))
df_events$d_max <- apply(diffs[,2:4], 1, max) #get intensification
df_events$p_max <- apply(diffs[,2:4], 1, which.max) # get intensification duration
df_events$is.fd <- (df_events$score > score)*(df_events$d_max > d_score)
# removes winter events
df_events$is.fd[lubridate::month(df_events$time) %in% c(1,2,11,12)] <- 0
#get correct length of the event (allow one week of recuperation (like christian))
i = 1
while (i < (nrow(df_events)-1)){
if ((df_events$is.fd[i] == 1 & df_events$score[i+1] > score) | (df_events$is.fd[i] == 1 & df_events$score[i+2] > score)){
df_events$is.fd[i+1] <-1
}
i <- i+1
}
# remove too short events (single week)
for (i in 2:(nrow(df_events) - 1)){
if (df_events$is.fd[i] == 1){
df_events$is.fd[i] <- max(df_events$is.fd[i-1], df_events$is.fd[i+1])
}
}
# add intensification period into event length
for (i in 1:nrow(df_events)){
if (df_events$is.fd[i] == 1){
df_events$is.fd[(i-df_events$p_max[i]):(i-1)] <- 1
}
}
index_aux3 <- rle(df_events$is.fd)$lengths
index_aux4 <- runner::runner(index_aux3, f = sum)
onset <- index_aux4[seq(0,length(index_aux4),2)]
duration <- index_aux3[seq(0,length(index_aux3),2)]
#join events that are 3 or less weeks apart
# index_aux3 <- index_aux3[seq(1,length(index_aux3),2)]
# index_aux4 <- index_aux4[seq(1,length(index_aux4),2)]
#
# if (min(index_aux3) < 4){
#   length_short_breaks <- index_aux3[which(index_aux3 < 4)]
#   position_short_breaks <- index_aux4[which(index_aux3 < 4)]
#
#   for (i in 1:length(length_short_breaks)){
#     i = 1
#     beg_break <- position_short_breaks[i] - length_short_breaks[i] + 1
#     for (j in beg_break:position_short_breaks[i]){
#       df_events$is.fd[j] <- 1
#     }
#   }
# }
# View(df_events)
#get summary data frame
fd.summary <- df_events[onset,]
if (nrow(fd.summary > 0)){
fd.summary$duration <- duration
fd.summary$event <- 1:nrow(fd.summary)
fd.summary$is.fd <- NULL
}
df_complete_data <- dplyr::left_join(df_week, df_events, by = 'time')
df_criteria <- cbind(df_criteria, df_events$is.fd)
output <- list(fd_timeseries = df_events, all_data = df_complete_data,
all_criteria = df_criteria, summary = fd.summary)
return(output)
}
test_multicriteria_fd(vtime =  df_d$time,
vtemp = df_d$temperature,
vprec = df_d$precipitation,
vet0 = ET0$et0,
veta = ETa$eta,
score = 0.6, d_score = 0.2,thresholds = c(1, 0, 0, 0, -2, 50, 10, 30))
